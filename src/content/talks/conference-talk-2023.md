---
title: "Scaling Laws and Efficient Training for Large Language Models"
event: "Neural Information Processing Systems (NeurIPS) 2023"
location: "New Orleans, LA, USA"
date: 2023-12-12
type: "conference"
# slides: "/slides/neurips-2023-scaling-laws.pdf"  # TODO: Add presentation slides
# poster: "/posters/neurips-2023-poster.pdf"  # TODO: Add poster PDF
tags: ["large-language-models", "scaling-laws", "efficiency", "deep-learning"]
abstract: "We investigate scaling laws for large language models and propose novel training techniques that achieve comparable performance with 40% less compute. Our analysis reveals insights into optimal model size, dataset size, and training duration trade-offs."
---

# Scaling Laws and Efficient Training for Large Language Models

**Neural Information Processing Systems (NeurIPS) 2023** | New Orleans, LA, USA | December 12, 2023

## Abstract

We investigate scaling laws for large language models and propose novel training techniques that achieve comparable performance with 40% less compute. Our analysis reveals insights into optimal model size, dataset size, and training duration trade-offs.

## Key Topics

- Extended scaling law analysis with interaction terms
- Progressive layerwise training technique
- Adaptive batch size scheduling
- Mixed precision with dynamic loss scaling
- Compute-optimal training strategies
- Experimental validation on models up to 6.7B parameters

## Resources

- Slides: Coming soon
- Poster: Coming soon
- Code: github.com/username/efficient-llm-training
